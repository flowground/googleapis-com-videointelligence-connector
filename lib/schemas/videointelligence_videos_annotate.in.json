{
    "type": "object",
    "properties": {
        "access_token": {
            "type": "string"
        },
        "alt": {
            "type": "string",
            "enum": [
                "json",
                "media",
                "proto"
            ],
            "default": "json"
        },
        "callback": {
            "type": "string"
        },
        "fields": {
            "type": "string"
        },
        "key": {
            "type": "string"
        },
        "oauth_token": {
            "type": "string"
        },
        "prettyPrint": {
            "type": "boolean",
            "default": true
        },
        "quotaUser": {
            "type": "string"
        },
        "uploadType": {
            "type": "string"
        },
        "upload_protocol": {
            "type": "string"
        },
        "requestBody": {
            "description": "Video annotation request.",
            "properties": {
                "features": {
                    "description": "Requested video annotation features.",
                    "items": {
                        "enum": [
                            "FEATURE_UNSPECIFIED",
                            "LABEL_DETECTION",
                            "SHOT_CHANGE_DETECTION",
                            "EXPLICIT_CONTENT_DETECTION",
                            "SPEECH_TRANSCRIPTION",
                            "TEXT_DETECTION",
                            "OBJECT_TRACKING"
                        ],
                        "type": "string"
                    },
                    "type": "array"
                },
                "inputContent": {
                    "description": "The video data bytes.\nIf unset, the input video(s) should be specified via `input_uri`.\nIf set, `input_uri` should be unset.",
                    "format": "byte",
                    "type": "string"
                },
                "inputUri": {
                    "description": "Input video location. Currently, only\n[Google Cloud Storage](https://cloud.google.com/storage/) URIs are\nsupported, which must be specified in the following format:\n`gs://bucket-id/object-id` (other URI formats return\ngoogle.rpc.Code.INVALID_ARGUMENT). For more information, see\n[Request URIs](/storage/docs/reference-uris).\nA video URI may include wildcards in `object-id`, and thus identify\nmultiple videos. Supported wildcards: '*' to match 0 or more characters;\n'?' to match 1 character. If unset, the input video should be embedded\nin the request as `input_content`. If set, `input_content` should be unset.",
                    "type": "string"
                },
                "locationId": {
                    "description": "Optional cloud region where annotation should take place. Supported cloud\nregions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region\nis specified, a region will be determined based on video file location.",
                    "type": "string"
                },
                "outputUri": {
                    "description": "Optional location where the output (in JSON format) should be stored.\nCurrently, only [Google Cloud Storage](https://cloud.google.com/storage/)\nURIs are supported, which must be specified in the following format:\n`gs://bucket-id/object-id` (other URI formats return\ngoogle.rpc.Code.INVALID_ARGUMENT). For more information, see\n[Request URIs](/storage/docs/reference-uris).",
                    "type": "string"
                },
                "videoContext": {
                    "description": "Additional video context and/or feature-specific parameters.",
                    "properties": {
                        "explicitContentDetectionConfig": {
                            "description": "Config for EXPLICIT_CONTENT_DETECTION.",
                            "properties": {
                                "model": {
                                    "description": "Model to use for explicit content detection.\nSupported values: \"builtin/stable\" (the default if unset) and\n\"builtin/latest\".",
                                    "type": "string"
                                }
                            },
                            "type": "object"
                        },
                        "labelDetectionConfig": {
                            "description": "Config for LABEL_DETECTION.",
                            "properties": {
                                "frameConfidenceThreshold": {
                                    "description": "The confidence threshold we perform filtering on the labels from\nframe-level detection. If not set, it is set to 0.4 by default. The valid\nrange for this threshold is [0.1, 0.9]. Any value set outside of this\nrange will be clipped.\nNote: for best results please follow the default threshold. We will update\nthe default threshold everytime when we release a new model.",
                                    "format": "float",
                                    "type": "number"
                                },
                                "labelDetectionMode": {
                                    "description": "What labels should be detected with LABEL_DETECTION, in addition to\nvideo-level labels or segment-level labels.\nIf unspecified, defaults to `SHOT_MODE`.",
                                    "enum": [
                                        "LABEL_DETECTION_MODE_UNSPECIFIED",
                                        "SHOT_MODE",
                                        "FRAME_MODE",
                                        "SHOT_AND_FRAME_MODE"
                                    ],
                                    "type": "string"
                                },
                                "model": {
                                    "description": "Model to use for label detection.\nSupported values: \"builtin/stable\" (the default if unset) and\n\"builtin/latest\".",
                                    "type": "string"
                                },
                                "stationaryCamera": {
                                    "description": "Whether the video has been shot from a stationary (i.e. non-moving) camera.\nWhen set to true, might improve detection accuracy for moving objects.\nShould be used with `SHOT_AND_FRAME_MODE` enabled.",
                                    "type": "boolean"
                                },
                                "videoConfidenceThreshold": {
                                    "description": "The confidence threshold we perform filtering on the labels from\nvideo-level and shot-level detections. If not set, it is set to 0.3 by\ndefault. The valid range for this threshold is [0.1, 0.9]. Any value set\noutside of this range will be clipped.\nNote: for best results please follow the default threshold. We will update\nthe default threshold everytime when we release a new model.",
                                    "format": "float",
                                    "type": "number"
                                }
                            },
                            "type": "object"
                        },
                        "segments": {
                            "description": "Video segments to annotate. The segments may overlap and are not required\nto be contiguous or span the whole video. If unspecified, each video is\ntreated as a single segment.",
                            "items": {
                                "description": "Video segment.",
                                "properties": {
                                    "endTimeOffset": {
                                        "description": "Time-offset, relative to the beginning of the video,\ncorresponding to the end of the segment (inclusive).",
                                        "format": "google-duration",
                                        "type": "string"
                                    },
                                    "startTimeOffset": {
                                        "description": "Time-offset, relative to the beginning of the video,\ncorresponding to the start of the segment (inclusive).",
                                        "format": "google-duration",
                                        "type": "string"
                                    }
                                },
                                "type": "object"
                            },
                            "type": "array"
                        },
                        "shotChangeDetectionConfig": {
                            "description": "Config for SHOT_CHANGE_DETECTION.",
                            "properties": {
                                "model": {
                                    "description": "Model to use for shot change detection.\nSupported values: \"builtin/stable\" (the default if unset) and\n\"builtin/latest\".",
                                    "type": "string"
                                }
                            },
                            "type": "object"
                        },
                        "speechTranscriptionConfig": {
                            "description": "Config for SPEECH_TRANSCRIPTION.",
                            "properties": {
                                "audioTracks": {
                                    "description": "*Optional* For file formats, such as MXF or MKV, supporting multiple audio\ntracks, specify up to two tracks. Default: track 0.",
                                    "items": {
                                        "format": "int32",
                                        "type": "integer"
                                    },
                                    "type": "array"
                                },
                                "diarizationSpeakerCount": {
                                    "description": "*Optional*\nIf set, specifies the estimated number of speakers in the conversation.\nIf not set, defaults to '2'.\nIgnored unless enable_speaker_diarization is set to true.",
                                    "format": "int32",
                                    "type": "integer"
                                },
                                "enableAutomaticPunctuation": {
                                    "description": "*Optional* If 'true', adds punctuation to recognition result hypotheses.\nThis feature is only available in select languages. Setting this for\nrequests in other languages has no effect at all. The default 'false' value\ndoes not add punctuation to result hypotheses. NOTE: \"This is currently\noffered as an experimental service, complimentary to all users. In the\nfuture this may be exclusively available as a premium feature.\"",
                                    "type": "boolean"
                                },
                                "enableSpeakerDiarization": {
                                    "description": "*Optional* If 'true', enables speaker detection for each recognized word in\nthe top alternative of the recognition result using a speaker_tag provided\nin the WordInfo.\nNote: When this is true, we send all the words from the beginning of the\naudio for the top alternative in every consecutive responses.\nThis is done in order to improve our speaker tags as our models learn to\nidentify the speakers in the conversation over time.",
                                    "type": "boolean"
                                },
                                "enableWordConfidence": {
                                    "description": "*Optional* If `true`, the top result includes a list of words and the\nconfidence for those words. If `false`, no word-level confidence\ninformation is returned. The default is `false`.",
                                    "type": "boolean"
                                },
                                "filterProfanity": {
                                    "description": "*Optional* If set to `true`, the server will attempt to filter out\nprofanities, replacing all but the initial character in each filtered word\nwith asterisks, e.g. \"f***\". If set to `false` or omitted, profanities\nwon't be filtered out.",
                                    "type": "boolean"
                                },
                                "languageCode": {
                                    "description": "*Required* The language of the supplied audio as a\n[BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.\nExample: \"en-US\".\nSee [Language Support](https://cloud.google.com/speech/docs/languages)\nfor a list of the currently supported language codes.",
                                    "type": "string"
                                },
                                "maxAlternatives": {
                                    "description": "*Optional* Maximum number of recognition hypotheses to be returned.\nSpecifically, the maximum number of `SpeechRecognitionAlternative` messages\nwithin each `SpeechTranscription`. The server may return fewer than\n`max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will\nreturn a maximum of one. If omitted, will return a maximum of one.",
                                    "format": "int32",
                                    "type": "integer"
                                },
                                "speechContexts": {
                                    "description": "*Optional* A means to provide context to assist the speech recognition.",
                                    "items": {
                                        "description": "Provides \"hints\" to the speech recognizer to favor specific words and phrases\nin the results.",
                                        "properties": {
                                            "phrases": {
                                                "description": "*Optional* A list of strings containing words and phrases \"hints\" so that\nthe speech recognition is more likely to recognize them. This can be used\nto improve the accuracy for specific words and phrases, for example, if\nspecific commands are typically spoken by the user. This can also be used\nto add additional words to the vocabulary of the recognizer. See\n[usage limits](https://cloud.google.com/speech/limits#content).",
                                                "items": {
                                                    "type": "string"
                                                },
                                                "type": "array"
                                            }
                                        },
                                        "type": "object"
                                    },
                                    "type": "array"
                                }
                            },
                            "type": "object"
                        },
                        "textDetectionConfig": {
                            "description": "Config for TEXT_DETECTION.",
                            "properties": {
                                "languageHints": {
                                    "description": "Language hint can be specified if the language to be detected is known a\npriori. It can increase the accuracy of the detection. Language hint must\nbe language code in BCP-47 format.\n\nAutomatic language detection is performed if no hint is provided.",
                                    "items": {
                                        "type": "string"
                                    },
                                    "type": "array"
                                }
                            },
                            "type": "object"
                        }
                    },
                    "type": "object"
                }
            },
            "type": "object"
        },
        "__xgafv": {
            "type": "string",
            "enum": [
                "1",
                "2"
            ]
        }
    }
}